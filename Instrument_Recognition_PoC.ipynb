{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéµ Instrument Recognition PoC\n",
    "\n",
    "**Goal**: Identify specific instruments present in audio using MIDI transcription + timbre matching\n",
    "\n",
    "**Pipeline**:\n",
    "- **Phase 0** (This notebook): MIDI transcription with YourMT3 ‚úÖ\n",
    "- **Phase 1**: YAMNet setup and instrument mapping (521 ‚Üí 25 categories) üöß\n",
    "- **Phase 2**: Note isolation from MIDI piano roll üöß\n",
    "- **Phase 3**: Timbre matching with YAMNet üöß\n",
    "- **Phase 4**: Output generation (aggregated % + timeline) üöß\n",
    "- **Phase 5**: Accuracy evaluation üöß\n",
    "\n",
    "**Reference**: See `instrument_recognition/SPECIFICATION.md` for full technical details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è IMPORTANT: Run cells in order (1‚Üí2‚Üí3‚Üí4‚Üí5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# Change to yourmt3_space directory\n",
    "original_dir = os.getcwd()\n",
    "if not os.path.exists('yourmt3_space'):\n",
    "    print(\"‚ùå Error: yourmt3_space directory not found!\")\n",
    "    print(\"   Run setup_yourmt3_brev.sh first\")\n",
    "else:\n",
    "    os.chdir('yourmt3_space')\n",
    "    sys.path.insert(0, '.')\n",
    "    sys.path.insert(0, 'amt/src')\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pretty_midi\n",
    "from IPython.display import Audio, display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from model_helper import load_model_checkpoint, transcribe\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"   Working directory: {os.getcwd()}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load YourMT3 Model\n",
    "\n",
    "**Model**: YPTF.MoE+Multi (noPS) - 536M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading YourMT3 model...\")\n",
    "print(\"This may take 10-15 seconds...\")\n",
    "\n",
    "# Model configuration\n",
    "checkpoint = \"mc13_256_g4_all_v7_mt3f_sqr_rms_moe_wf4_n8k2_silu_rope_rp_b36_nops@last.ckpt\"\n",
    "project = '2024'\n",
    "precision = '16'\n",
    "\n",
    "args = [\n",
    "    checkpoint,\n",
    "    '-p', project,\n",
    "    '-tk', 'mc13_full_plus_256',\n",
    "    '-dec', 'multi-t5',\n",
    "    '-nl', '26',\n",
    "    '-enc', 'perceiver-tf',\n",
    "    '-sqr', '1',\n",
    "    '-ff', 'moe',\n",
    "    '-wf', '4',\n",
    "    '-nmoe', '8',\n",
    "    '-kmoe', '2',\n",
    "    '-act', 'silu',\n",
    "    '-epe', 'rope',\n",
    "    '-rp', '1',\n",
    "    '-ac', 'spec',\n",
    "    '-hop', '300',\n",
    "    '-atc', '1',\n",
    "    '-pr', precision\n",
    "]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = load_model_checkpoint(args=args, device=device)\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Model: YPTF.MoE+Multi (noPS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions - Phase 0 (MIDI Transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_midi(midi_path):\n",
    "    \"\"\"Analyze MIDI file and return statistics\"\"\"\n",
    "    midi = pretty_midi.PrettyMIDI(midi_path)\n",
    "    \n",
    "    stats = {\n",
    "        'total_notes': sum(len(inst.notes) for inst in midi.instruments),\n",
    "        'num_instruments': len(midi.instruments),\n",
    "        'duration': midi.get_end_time(),\n",
    "        'instruments': []\n",
    "    }\n",
    "    \n",
    "    for i, inst in enumerate(midi.instruments):\n",
    "        if len(inst.notes) > 0:\n",
    "            stats['instruments'].append({\n",
    "                'index': i,\n",
    "                'program': inst.program,\n",
    "                'name': pretty_midi.program_to_instrument_name(inst.program),\n",
    "                'notes': len(inst.notes),\n",
    "                'is_drum': inst.is_drum\n",
    "            })\n",
    "    \n",
    "    return stats, midi\n",
    "\n",
    "def transcribe_audio(audio_path, output_name):\n",
    "    \"\"\"Transcribe audio and return MIDI path + stats\"\"\"\n",
    "    info = torchaudio.info(audio_path)\n",
    "    duration = info.num_frames / info.sample_rate\n",
    "    \n",
    "    audio_info = {\n",
    "        \"filepath\": audio_path,\n",
    "        \"track_name\": output_name,\n",
    "        \"sample_rate\": int(info.sample_rate),\n",
    "        \"bits_per_sample\": int(info.bits_per_sample) if info.bits_per_sample else 16,\n",
    "        \"num_channels\": int(info.num_channels),\n",
    "        \"num_frames\": int(info.num_frames),\n",
    "        \"duration\": int(duration),\n",
    "        \"encoding\": 'unknown',\n",
    "    }\n",
    "    \n",
    "    # Transcribe\n",
    "    midi_path = transcribe(model, audio_info)\n",
    "    \n",
    "    # Analyze\n",
    "    stats, midi = analyze_midi(midi_path)\n",
    "    \n",
    "    return midi_path, stats, midi\n",
    "\n",
    "def midi_to_audio(midi_path, sample_rate=16000):\n",
    "    \"\"\"Convert MIDI to audio for playback using FluidSynth\"\"\"\n",
    "    try:\n",
    "        midi = pretty_midi.PrettyMIDI(midi_path)\n",
    "        audio = midi.fluidsynth(fs=sample_rate)\n",
    "        return audio, sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  MIDI synthesis failed: {e}\")\n",
    "        print(\"   Note: FluidSynth may not be installed\")\n",
    "        return None, None\n",
    "\n",
    "def plot_piano_roll(midi, title, ax=None):\n",
    "    \"\"\"Plot piano roll from MIDI object\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    # Get all notes\n",
    "    notes_to_plot = []\n",
    "    colors = plt.cm.tab20.colors\n",
    "    \n",
    "    for inst_idx, inst in enumerate(midi.instruments):\n",
    "        color = colors[inst_idx % len(colors)]\n",
    "        for note in inst.notes:\n",
    "            notes_to_plot.append({\n",
    "                'start': note.start,\n",
    "                'end': note.end,\n",
    "                'pitch': note.pitch,\n",
    "                'velocity': note.velocity,\n",
    "                'color': color,\n",
    "                'instrument': inst.program,\n",
    "                'is_drum': inst.is_drum\n",
    "            })\n",
    "    \n",
    "    # Plot notes\n",
    "    for note_info in notes_to_plot:\n",
    "        ax.add_patch(\n",
    "            plt.Rectangle(\n",
    "                (note_info['start'], note_info['pitch']),\n",
    "                note_info['end'] - note_info['start'],\n",
    "                1,\n",
    "                facecolor=note_info['color'],\n",
    "                edgecolor='black',\n",
    "                linewidth=0.5,\n",
    "                alpha=0.7\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    ax.set_xlim(0, midi.get_end_time())\n",
    "    ax.set_ylim(20, 108)  # Piano range\n",
    "    ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax.set_ylabel('MIDI Pitch', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add note count\n",
    "    total_notes = len(notes_to_plot)\n",
    "    ax.text(0.98, 0.02, f'{total_notes} notes', \n",
    "            transform=ax.transAxes,\n",
    "            ha='right', va='bottom',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "            fontsize=10)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find audio files\n",
    "audio_extensions = ['*.mp3', '*.wav', '*.flac', '*.m4a', '*.ogg']\n",
    "audio_files = []\n",
    "for ext in audio_extensions:\n",
    "    audio_files.extend(glob.glob(os.path.join(original_dir, ext)))\n",
    "\n",
    "if len(audio_files) == 0:\n",
    "    print(\"‚ùå No audio files found!\")\n",
    "    print(\"   Please upload audio files to the MT3 directory\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(audio_files)} audio files:\")\n",
    "    for i, f in enumerate(audio_files):\n",
    "        info = torchaudio.info(f)\n",
    "        duration = info.num_frames / info.sample_rate\n",
    "        print(f\"   {i+1}. {os.path.basename(f)} ({duration:.1f}s)\")\n",
    "\n",
    "# File selector\n",
    "file_selector = widgets.Dropdown(\n",
    "    options=[(f\"{os.path.basename(f)} ({torchaudio.info(f).num_frames/torchaudio.info(f).sample_rate:.1f}s)\", f) for f in audio_files],\n",
    "    description='Test File:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='700px')\n",
    ")\n",
    "\n",
    "display(file_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 0: MIDI Transcription\n",
    "\n",
    "**Goal**: Generate MIDI transcription from audio using YourMT3\n",
    "\n",
    "**Output**: MIDI file with multi-instrument transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global results storage\n",
    "phase0_results = {}\n",
    "\n",
    "def run_phase0(button):\n",
    "    global phase0_results\n",
    "    \n",
    "    output.clear_output(wait=True)\n",
    "    \n",
    "    with output:\n",
    "        audio_path = file_selector.value\n",
    "        \n",
    "        if not audio_path:\n",
    "            print(\"‚ùå Please select an audio file first!\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"üéØ PHASE 0: MIDI Transcription\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nüìÅ Input File: {os.path.basename(audio_path)}\")\n",
    "        \n",
    "        try:\n",
    "            # Transcribe\n",
    "            print(\"\\nüéµ Running YourMT3 transcription...\")\n",
    "            print(\"   This may take 30-60 seconds...\")\n",
    "            \n",
    "            midi_path, stats, midi = transcribe_audio(audio_path, \"instrument_recognition\")\n",
    "            \n",
    "            # Store results\n",
    "            phase0_results = {\n",
    "                'audio_path': audio_path,\n",
    "                'midi_path': midi_path,\n",
    "                'stats': stats,\n",
    "                'midi': midi\n",
    "            }\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\n‚úÖ Transcription Complete:\")\n",
    "            print(f\"   MIDI file: {midi_path}\")\n",
    "            print(f\"   Total notes: {stats['total_notes']}\")\n",
    "            print(f\"   Instruments detected: {stats['num_instruments']}\")\n",
    "            print(f\"   Duration: {stats['duration']:.2f}s\")\n",
    "            \n",
    "            # Show instrument breakdown\n",
    "            print(f\"\\nüé∏ Detected Instruments:\")\n",
    "            for inst in stats['instruments']:\n",
    "                drum_label = \"(Drums)\" if inst['is_drum'] else \"\"\n",
    "                print(f\"   - {inst['name']} {drum_label}: {inst['notes']} notes (Program {inst['program']})\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"‚úÖ PHASE 0 COMPLETE!\")\n",
    "            print(\"=\"*80)\n",
    "            print(\"\\nRun the cells below to:\")\n",
    "            print(\"   - View piano roll visualization\")\n",
    "            print(\"   - Listen to MIDI playback\")\n",
    "            print(\"   - Proceed to Phase 1 (YAMNet setup)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Transcription failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Create button and output\n",
    "phase0_button = widgets.Button(\n",
    "    description='üöÄ Run Phase 0',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px', height='50px')\n",
    ")\n",
    "phase0_button.on_click(run_phase0)\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "print(\"‚ö†Ô∏è  Note: Transcription takes ~30-60 seconds depending on audio length\")\n",
    "print(\"\\n‚úÖ Ready! Click the button below to start Phase 0\")\n",
    "\n",
    "display(phase0_button)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View MIDI Results\n",
    "\n",
    "*Run this after Phase 0 completes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if phase0_results:\n",
    "    print(\"üéπ MIDI Transcription Results\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    stats = phase0_results['stats']\n",
    "    midi = phase0_results['midi']\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Audio: {os.path.basename(phase0_results['audio_path'])}\")\n",
    "    print(f\"   MIDI: {phase0_results['midi_path']}\")\n",
    "    print(f\"   Total notes: {stats['total_notes']}\")\n",
    "    print(f\"   Instruments: {stats['num_instruments']}\")\n",
    "    print(f\"   Duration: {stats['duration']:.2f}s\")\n",
    "    \n",
    "    # Piano roll visualization\n",
    "    print(f\"\\nüéπ Piano Roll:\")\n",
    "    fig, ax = plt.subplots(figsize=(18, 6))\n",
    "    plot_piano_roll(midi, 'üéµ MIDI Transcription - Phase 0', ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # MIDI playback\n",
    "    print(f\"\\nüîä MIDI Playback:\")\n",
    "    audio, sr = midi_to_audio(phase0_results['midi_path'])\n",
    "    if audio is not None:\n",
    "        display(Audio(audio, rate=sr))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Phase 0 complete!\")\n",
    "    print(\"\\nüí° Next Steps:\")\n",
    "    print(\"   ‚Üí Phase 1: Set up YAMNet for timbre matching\")\n",
    "    print(\"   ‚Üí Phase 2: Extract isolated notes from MIDI piano roll\")\n",
    "    print(\"   ‚Üí Phase 3: Match notes to instrument timbres\")\n",
    "    print(\"   ‚Üí Phase 4: Generate aggregated % and timeline outputs\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Run Phase 0 first (Cell 5)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üéØ Phase 1: YAMNet Setup & Instrument Mapping\n\n**Goal**: Load YAMNet model and create instrument mapping\n\n**Tasks**:\n1. Install TensorFlow and YAMNet dependencies ‚úÖ\n2. Load pretrained YAMNet model ‚úÖ\n3. Create mapping: 521 AudioSet classes ‚Üí 25 Level 2 instrument categories\n4. Test YAMNet on sample audio segments ‚úÖ"
  },
  {
   "cell_type": "markdown",
   "source": "### 7.1. Import Phase 1 Dependencies\n\n‚ö†Ô∏è **Prerequisite**: Run `pip install -r instrument_recognition/requirements.txt` on the instance first!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 7.5. Create Instrument Mapping File\n\n**User Task**: Create `yamnet_to_level2_mapping.json` to map 521 YAMNet classes ‚Üí 25 Level 2 categories.\n\n**Instructions**:\n1. Review YAMNet classes above and SPECIFICATION.md for Level 2 categories\n2. Create mapping file: `instrument_recognition/yamnet_to_level2_mapping.json`\n3. Format: `{\"YAMNet_Class_Name\": \"Level2_Category\", ...}`\n\n**Example mapping**:\n```json\n{\n  \"Bass drum\": \"Kick Drum\",\n  \"Snare drum\": \"Snare Drum\",\n  \"Hi-hat\": \"Hi-hat (Closed)\",\n  \"Cymbal\": \"Crash Cymbal\",\n  \"Electric guitar\": \"Electric Guitar (Clean)\",\n  \"Distortion\": \"Electric Guitar (Distorted)\",\n  \"Acoustic guitar\": \"Acoustic Guitar\",\n  \"Bass guitar\": \"Electric Bass\",\n  \"Synthesizer\": \"Synthesizer (Lead)\",\n  \"Piano\": \"Piano (Acoustic)\",\n  \"Electric piano\": \"Electric Piano\",\n  ...\n}\n```\n\n**Level 2 Categories (25 total)**:\n- **Drums**: Kick Drum, Snare Drum, Hi-hat (Closed), Hi-hat (Open), Crash Cymbal, Ride Cymbal, Tom Drum, Electronic Drum\n- **Bass**: Electric Bass, Synth Bass, Acoustic Bass  \n- **Guitar**: Electric Guitar (Clean), Electric Guitar (Distorted), Acoustic Guitar\n- **Keys**: Piano (Acoustic), Electric Piano, Synthesizer (Lead), Synthesizer (Pad)\n- **Other**: Orchestral Strings, Brass, Woodwinds, Vocals, Vocal Sample, Sound Effects, Unknown\n\nOnce created, the mapping will be loaded in Phase 3 for timbre matching.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if phase0_results:\n    print(\"üß™ Testing YAMNet on audio from Phase 0...\")\n    \n    # Load audio\n    audio_path = phase0_results['audio_path']\n    waveform, sr = librosa.load(audio_path, sr=16000, mono=True, duration=5.0)  # First 5 seconds\n    \n    print(f\"   Audio: {os.path.basename(audio_path)}\")\n    print(f\"   Sample rate: {sr} Hz\")\n    print(f\"   Duration: {len(waveform)/sr:.2f}s\")\n    \n    # Run YAMNet inference\n    print(\"\\nüîç Running YAMNet classification...\")\n    scores, embeddings, spectrogram = yamnet_model(waveform)\n    \n    # Average scores across time\n    mean_scores = np.mean(scores.numpy(), axis=0)\n    \n    # Get top-10 predictions\n    top_k = 10\n    top_indices = np.argsort(mean_scores)[-top_k:][::-1]\n    \n    print(f\"\\n‚úÖ Top {top_k} predicted classes:\")\n    for i, idx in enumerate(top_indices, 1):\n        print(f\"   {i:2d}. {class_names[idx]:30s} (confidence: {mean_scores[idx]:.3f})\")\n    \n    print(\"\\nüí° These raw YAMNet predictions will be mapped to Level 2 categories\")\n    \nelse:\n    print(\"‚ö†Ô∏è  Run Phase 0 first to get audio for testing!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.4. Test YAMNet on Sample Audio\n\nTest YAMNet classification on a short audio segment from Phase 0.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Browse instrument-related classes\nprint(\"üé∏ Instrument-Related AudioSet Classes (Sample):\\n\")\n\n# Common instrument keywords\ninstrument_keywords = ['guitar', 'piano', 'drum', 'bass', 'synth', 'kick', 'snare', 'cymbal', \n                       'violin', 'trumpet', 'saxophone', 'flute', 'vocal', 'voice']\n\ninstrument_classes = []\nfor idx, name in enumerate(class_names):\n    for keyword in instrument_keywords:\n        if keyword.lower() in name.lower():\n            instrument_classes.append((idx, name))\n            break\n\nprint(f\"Found {len(instrument_classes)} instrument-related classes out of {len(class_names)} total:\\n\")\n\n# Show first 30\nfor idx, name in instrument_classes[:30]:\n    print(f\"   [{idx:3d}] {name}\")\n\nif len(instrument_classes) > 30:\n    print(f\"\\n   ... and {len(instrument_classes) - 30} more\")\n    \nprint(f\"\\nüí° These classes will be mapped to 25 Level 2 categories in the next step\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.3. Browse YAMNet AudioSet Classes\n\nExplore the 521 AudioSet classes to understand what YAMNet can detect.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Fix: Import pandas for class name loading\nimport pandas as pd\n\nprint(\"üì• Loading YAMNet model from TensorFlow Hub...\")\nprint(\"   This will download ~4MB on first run...\")\n\ntry:\n    # Load YAMNet model\n    yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n    \n    # Load class names\n    class_map_path = yamnet_model.class_map_path().numpy()\n    class_names = list(pd.read_csv(class_map_path)['display_name'])\n    \n    print(f\"\\n‚úÖ YAMNet loaded successfully!\")\n    print(f\"   Total AudioSet classes: {len(class_names)}\")\n    print(f\"   Model inputs: 16kHz mono audio (variable length)\")\n    print(f\"   Model outputs: 521-dim probability vector + 1024-dim embeddings\")\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå YAMNet loading failed: {e}\")\n    import traceback\n    traceback.print_exc()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"üì• Loading YAMNet model from TensorFlow Hub...\")\nprint(\"   This will download ~4MB on first run...\")\n\ntry:\n    # Load YAMNet model\n    yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n    \n    # Load class names\n    class_map_path = yamnet_model.class_map_path().numpy()\n    class_names = list(pd.read_csv(class_map_path)['display_name'])\n    \n    print(f\"\\n‚úÖ YAMNet loaded successfully!\")\n    print(f\"   Total AudioSet classes: {len(class_names)}\")\n    print(f\"   Model inputs: 16kHz mono audio (variable length)\")\n    print(f\"   Model outputs: 521-dim probability vector + 1024-dim embeddings\")\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå YAMNet loading failed: {e}\")\n    import traceback\n    traceback.print_exc()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2. Load YAMNet Model\n\nLoad pretrained YAMNet model from TensorFlow Hub (auto-downloads ~4MB)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import TensorFlow and YAMNet dependencies\ntry:\n    import tensorflow as tf\n    import tensorflow_hub as hub\n    import librosa\n    \n    print(\"‚úÖ TensorFlow imports successful!\")\n    print(f\"   TensorFlow version: {tf.__version__}\")\n    print(f\"   TensorFlow Hub version: {hub.__version__}\")\n    print(f\"   Librosa version: {librosa.__version__}\")\n    \n    # Check for GPU\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        print(f\"   TensorFlow GPU: {len(gpus)} GPU(s) available\")\n    else:\n        print(\"   TensorFlow GPU: Running on CPU\")\n        \nexcept ImportError as e:\n    print(f\"‚ùå Import failed: {e}\")\n    print(\"\\nüì¶ Please install dependencies first:\")\n    print(\"   Run on instance terminal:\")\n    print(\"   pip install -r instrument_recognition/requirements.txt\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöß Phase 2: Note Isolation (Coming Next)\n",
    "\n",
    "**Goal**: Extract isolated notes from MIDI for timbre matching\n",
    "\n",
    "**Tasks**:\n",
    "1. Parse MIDI piano roll to identify individual notes\n",
    "2. Extract corresponding audio segments from original audio\n",
    "3. Score isolation quality (temporal, pitch, energy)\n",
    "4. Select top N isolated notes per MIDI instrument\n",
    "\n",
    "**Status**: Not implemented yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöß Phase 3: Timbre Matching (Coming Next)\n",
    "\n",
    "**Goal**: Match isolated notes to real instrument timbres using YAMNet\n",
    "\n",
    "**Tasks**:\n",
    "1. Run YAMNet inference on isolated audio segments\n",
    "2. Aggregate predictions per MIDI instrument\n",
    "3. Vote on most likely real instrument match\n",
    "4. Generate confidence scores\n",
    "\n",
    "**Status**: Not implemented yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöß Phase 4: Output Generation (Coming Next)\n",
    "\n",
    "**Goal**: Create final outputs - aggregated % and timeline\n",
    "\n",
    "**Tasks**:\n",
    "1. **Output B**: Aggregated instrument percentages (e.g., \"Electric Guitar: 35%, Piano: 25%, ...\")\n",
    "2. **Output C**: Timeline of instrument presence (e.g., \"0-30s: Piano, 30-60s: Guitar + Drums\")\n",
    "3. Visualization: Piano roll with identified instruments\n",
    "4. Save results to JSON/CSV\n",
    "\n",
    "**Status**: Not implemented yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöß Phase 5: Evaluation (Coming Next)\n",
    "\n",
    "**Goal**: Assess accuracy and tune thresholds\n",
    "\n",
    "**Tasks**:\n",
    "1. Manual evaluation against ground truth\n",
    "2. Calculate accuracy metrics\n",
    "3. Error analysis (common misclassifications)\n",
    "4. Threshold tuning for optimal performance\n",
    "\n",
    "**Target**: 85% accuracy, 80% instrument coverage\n",
    "\n",
    "**Status**: Not implemented yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Implementation Checklist\n",
    "\n",
    "**Phase 0** (Current):\n",
    "- ‚úÖ YourMT3 model loading\n",
    "- ‚úÖ Audio file selection\n",
    "- ‚úÖ MIDI transcription\n",
    "- ‚úÖ MIDI analysis and visualization\n",
    "\n",
    "**Phase 1** (Next):\n",
    "- ‚¨ú Install TensorFlow + YAMNet\n",
    "- ‚¨ú Load pretrained YAMNet model\n",
    "- ‚¨ú Create 521‚Üí25 instrument mapping\n",
    "- ‚¨ú Test YAMNet inference\n",
    "\n",
    "**Phase 2-5**:\n",
    "- ‚¨ú Note isolation algorithm\n",
    "- ‚¨ú Isolation quality scoring\n",
    "- ‚¨ú YAMNet timbre matching\n",
    "- ‚¨ú Aggregated output generation\n",
    "- ‚¨ú Timeline output generation\n",
    "- ‚¨ú Visualization\n",
    "- ‚¨ú Accuracy evaluation\n",
    "\n",
    "**Reference**: See `instrument_recognition/SPECIFICATION.md` for detailed technical specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Assisted by Claude Code*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}