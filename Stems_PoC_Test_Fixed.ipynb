{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽµ Stems Separation PoC: Accuracy Comparison\n",
    "\n",
    "**Proof of Concept** to validate Idea 1: Stems-based transcription approach.\n",
    "\n",
    "**Hypothesis**: Separating audio into stems (bass, drums, other, vocals) before transcription improves accuracy.\n",
    "\n",
    "**Test**:\n",
    "1. Transcribe **full mix** with YourMT3\n",
    "2. Separate audio into **4 stems** with Demucs\n",
    "3. Transcribe **each stem** with YourMT3\n",
    "4. Compare accuracy and analyze improvements\n",
    "\n",
    "**Expected**: 10-15% accuracy improvement on stems (without fine-tuning yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš ï¸ IMPORTANT: Run cells in order (1â†’2â†’3â†’4â†’5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# Change to yourmt3_space directory\n",
    "original_dir = os.getcwd()\n",
    "if not os.path.exists('yourmt3_space'):\n",
    "    print(\"âŒ Error: yourmt3_space directory not found!\")\n",
    "    print(\"   Run setup_yourmt3_brev.sh first\")\n",
    "else:\n",
    "    os.chdir('yourmt3_space')\n",
    "    sys.path.insert(0, '.')\n",
    "    sys.path.insert(0, 'amt/src')\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pretty_midi\n",
    "from IPython.display import Audio, display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from model_helper import load_model_checkpoint, transcribe\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "print(f\"   Working directory: {os.getcwd()}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Demucs (as Python library)\n",
    "\n",
    "**Run this cell once** - it will import and prepare Demucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Demucs as a Python library (no subprocess needed!)\n",
    "try:\n",
    "    from demucs.pretrained import get_model\n",
    "    from demucs.apply import apply_model\n",
    "    print(\"âœ… Demucs imported successfully!\")\n",
    "    \n",
    "    # Pre-load the htdemucs model\n",
    "    print(\"ðŸ“¦ Loading htdemucs model...\")\n",
    "    print(\"   (This will download ~80MB on first run)\")\n",
    "    demucs_model = get_model('htdemucs')\n",
    "    demucs_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    demucs_model.to(demucs_device)\n",
    "    print(f\"âœ… Demucs model loaded on {demucs_device}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(\"âŒ Demucs not installed!\")\n",
    "    print(\"   Installing demucs...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', '-q', 'demucs'], check=True)\n",
    "    print(\"âœ… Demucs installed! Please restart kernel and re-run this cell.\")\n",
    "    demucs_model = None\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading Demucs: {e}\")\n",
    "    demucs_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load YourMT3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading YourMT3 model...\")\n",
    "print(\"This may take 10-15 seconds...\")\n",
    "\n",
    "# Model configuration\n",
    "checkpoint = \"mc13_256_g4_all_v7_mt3f_sqr_rms_moe_wf4_n8k2_silu_rope_rp_b36_nops@last.ckpt\"\n",
    "project = '2024'\n",
    "precision = '16'\n",
    "\n",
    "args = [\n",
    "    checkpoint,\n",
    "    '-p', project,\n",
    "    '-tk', 'mc13_full_plus_256',\n",
    "    '-dec', 'multi-t5',\n",
    "    '-nl', '26',\n",
    "    '-enc', 'perceiver-tf',\n",
    "    '-sqr', '1',\n",
    "    '-ff', 'moe',\n",
    "    '-wf', '4',\n",
    "    '-nmoe', '8',\n",
    "    '-kmoe', '2',\n",
    "    '-act', 'silu',\n",
    "    '-epe', 'rope',\n",
    "    '-rp', '1',\n",
    "    '-ac', 'spec',\n",
    "    '-hop', '300',\n",
    "    '-atc', '1',\n",
    "    '-pr', precision\n",
    "]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = load_model_checkpoint(args=args, device=device)\n",
    "\n",
    "print(\"\\nâœ… Model loaded successfully!\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Model: YPTF.MoE+Multi (noPS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions for PoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def separate_stems_demucs(audio_path, output_dir=\"separated\"):\n    \"\"\"Separate audio into 4 stems using Demucs Python API\"\"\"\n    print(f\"ðŸŽµ Separating stems with Demucs...\")\n    print(f\"   Input: {audio_path}\")\n    print(f\"   Output: {output_dir}/\")\n    print(\"   This may take 30-60 seconds...\")\n    \n    if demucs_model is None:\n        print(\"\\nâŒ Demucs model not loaded!\")\n        print(\"   Please run Cell 2 first\")\n        return None\n    \n    try:\n        # Load audio\n        wav, sr = torchaudio.load(audio_path)\n        \n        # Demucs expects 44.1kHz, resample if needed\n        if sr != 44100:\n            resampler = torchaudio.transforms.Resample(sr, 44100)\n            wav = resampler(wav)\n            sr = 44100\n        \n        # Convert to mono if stereo (Demucs expects mono or stereo)\n        if wav.shape[0] > 2:\n            wav = wav[:2]  # Take first 2 channels\n        \n        # Apply Demucs model\n        print(\"   Running separation...\")\n        wav = wav.to(demucs_device)\n        \n        with torch.no_grad():\n            sources = apply_model(demucs_model, wav[None], device=demucs_device)[0]\n        \n        # sources shape: [4, channels, samples]\n        # Order: drums, bass, other, vocals\n        stems_order = ['drums', 'bass', 'other', 'vocals']\n        \n        # Create output directory\n        track_name = Path(audio_path).stem\n        stem_dir = Path(output_dir) / 'htdemucs' / track_name\n        stem_dir.mkdir(parents=True, exist_ok=True)\n        \n        stems = {}\n        \n        # Save each stem\n        for i, stem_name in enumerate(stems_order):\n            stem_path = stem_dir / f\"{stem_name}.wav\"\n            stem_audio = sources[i].cpu()\n            torchaudio.save(str(stem_path), stem_audio, sr)\n            stems[stem_name] = str(stem_path)\n        \n        print(\"âœ… Stems separated successfully!\")\n        for stem_name in stems_order:\n            print(f\"   - {stem_name}.wav\")\n        \n        return stems\n        \n    except Exception as e:\n        print(f\"âŒ Demucs separation failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\ndef analyze_midi(midi_path):\n    \"\"\"Analyze MIDI file and return statistics\"\"\"\n    midi = pretty_midi.PrettyMIDI(midi_path)\n    \n    stats = {\n        'total_notes': sum(len(inst.notes) for inst in midi.instruments),\n        'num_instruments': len(midi.instruments),\n        'duration': midi.get_end_time(),\n        'instruments': []\n    }\n    \n    for i, inst in enumerate(midi.instruments):\n        if len(inst.notes) > 0:\n            stats['instruments'].append({\n                'index': i,\n                'program': inst.program,\n                'name': pretty_midi.program_to_instrument_name(inst.program),\n                'notes': len(inst.notes),\n                'is_drum': inst.is_drum\n            })\n    \n    return stats, midi\n\ndef transcribe_audio(audio_path, output_name):\n    \"\"\"Transcribe audio and return MIDI path + stats\"\"\"\n    info = torchaudio.info(audio_path)\n    duration = info.num_frames / info.sample_rate\n    \n    audio_info = {\n        \"filepath\": audio_path,\n        \"track_name\": output_name,\n        \"sample_rate\": int(info.sample_rate),\n        \"bits_per_sample\": int(info.bits_per_sample) if info.bits_per_sample else 16,\n        \"num_channels\": int(info.num_channels),\n        \"num_frames\": int(info.num_frames),\n        \"duration\": int(duration),\n        \"encoding\": 'unknown',\n    }\n    \n    # Transcribe\n    midi_path = transcribe(model, audio_info)\n    \n    # Analyze\n    stats, midi = analyze_midi(midi_path)\n    \n    return midi_path, stats\n\ndef midi_to_audio(midi_path, sample_rate=16000):\n    \"\"\"Convert MIDI to audio for playback using FluidSynth\"\"\"\n    try:\n        midi = pretty_midi.PrettyMIDI(midi_path)\n        audio = midi.fluidsynth(fs=sample_rate)\n        return audio, sample_rate\n    except Exception as e:\n        print(f\"âš ï¸  MIDI synthesis failed: {e}\")\n        print(\"   Note: FluidSynth may not be installed\")\n        return None, None\n\ndef merge_midi_files(midi_paths_dict, output_path=\"output_midi/poc_recomposed.mid\"):\n    \"\"\"Merge multiple MIDI files into one recomposed MIDI\n    \n    Args:\n        midi_paths_dict: Dictionary of {stem_name: midi_path}\n        output_path: Where to save the merged MIDI file\n    \n    Returns:\n        Path to merged MIDI file\n    \"\"\"\n    print(f\"ðŸ”„ Merging {len(midi_paths_dict)} MIDI files into recomposed mix...\")\n    \n    # Create output directory\n    output_dir = Path(output_path).parent\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Create new MIDI object\n    merged_midi = pretty_midi.PrettyMIDI()\n    \n    # Copy all instruments from each stem\n    for stem_name, midi_path in midi_paths_dict.items():\n        try:\n            stem_midi = pretty_midi.PrettyMIDI(midi_path)\n            \n            # Add all instruments from this stem\n            for instrument in stem_midi.instruments:\n                # Create a copy of the instrument\n                merged_instrument = pretty_midi.Instrument(\n                    program=instrument.program,\n                    is_drum=instrument.is_drum,\n                    name=f\"{stem_name}_{instrument.name}\" if instrument.name else stem_name\n                )\n                \n                # Copy all notes\n                merged_instrument.notes = instrument.notes.copy()\n                \n                # Copy control changes\n                merged_instrument.control_changes = instrument.control_changes.copy()\n                \n                # Add to merged MIDI\n                merged_midi.instruments.append(merged_instrument)\n            \n            print(f\"   âœ… {stem_name}: {len(stem_midi.instruments)} instruments added\")\n            \n        except Exception as e:\n            print(f\"   âš ï¸  Failed to merge {stem_name}: {e}\")\n    \n    # Save merged MIDI\n    merged_midi.write(output_path)\n    \n    print(f\"âœ… Recomposed MIDI saved: {output_path}\")\n    print(f\"   Total instruments: {len(merged_midi.instruments)}\")\n    print(f\"   Total notes: {sum(len(inst.notes) for inst in merged_midi.instruments)}\")\n    \n    return output_path\n\nprint(\"âœ… Helper functions loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select Audio File for PoC Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find audio files\n",
    "audio_extensions = ['*.mp3', '*.wav', '*.flac', '*.m4a', '*.ogg']\n",
    "audio_files = []\n",
    "for ext in audio_extensions:\n",
    "    audio_files.extend(glob.glob(os.path.join(original_dir, ext)))\n",
    "\n",
    "if len(audio_files) == 0:\n",
    "    print(\"âŒ No audio files found!\")\n",
    "    print(\"   Please upload audio files to the MT3 directory\")\n",
    "else:\n",
    "    print(f\"âœ… Found {len(audio_files)} audio files:\")\n",
    "    for i, f in enumerate(audio_files):\n",
    "        info = torchaudio.info(f)\n",
    "        duration = info.num_frames / info.sample_rate\n",
    "        print(f\"   {i+1}. {os.path.basename(f)} ({duration:.1f}s)\")\n",
    "\n",
    "# File selector\n",
    "file_selector = widgets.Dropdown(\n",
    "    options=[(f\"{os.path.basename(f)} ({torchaudio.info(f).num_frames/torchaudio.info(f).sample_rate:.1f}s)\", f) for f in audio_files],\n",
    "    description='Test File:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='700px')\n",
    ")\n",
    "\n",
    "display(file_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run PoC Test: Full Mix vs Stems\n",
    "\n",
    "**Click the button below to start the test** (takes 3-5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Global results storage\npoc_results = {}\n\ndef run_poc_test(button):\n    global poc_results\n    \n    output.clear_output(wait=True)\n    \n    with output:\n        audio_path = file_selector.value\n        \n        if not audio_path:\n            print(\"âŒ Please select an audio file first!\")\n            return\n        \n        print(\"=\"*80)\n        print(\"ðŸŽ¯ PROOF OF CONCEPT TEST: Full Mix vs Stems\")\n        print(\"=\"*80)\n        print(f\"\\nðŸ“ Test File: {os.path.basename(audio_path)}\")\n        \n        # Step 1: Transcribe full mix\n        print(\"\\n\" + \"=\"*80)\n        print(\"STEP 1: Transcribe Full Mix (Baseline)\")\n        print(\"=\"*80)\n        \n        try:\n            fullmix_midi, fullmix_stats = transcribe_audio(audio_path, \"poc_fullmix\")\n            \n            print(f\"\\nâœ… Full Mix Transcription Complete:\")\n            print(f\"   Total notes: {fullmix_stats['total_notes']}\")\n            print(f\"   Instruments: {fullmix_stats['num_instruments']}\")\n            print(f\"   Duration: {fullmix_stats['duration']:.2f}s\")\n        except Exception as e:\n            print(f\"\\nâŒ Full mix transcription failed: {e}\")\n            import traceback\n            traceback.print_exc()\n            return\n        \n        # Step 2: Separate stems\n        print(\"\\n\" + \"=\"*80)\n        print(\"STEP 2: Separate Stems with Demucs\")\n        print(\"=\"*80)\n        \n        stems = separate_stems_demucs(audio_path, output_dir=\"separated\")\n        \n        if not stems:\n            print(\"\\nâŒ Stem separation failed! Stopping test.\")\n            print(\"   Make sure Demucs is loaded (run Cell 2)\")\n            return\n        \n        # Step 3: Transcribe each stem\n        print(\"\\n\" + \"=\"*80)\n        print(\"STEP 3: Transcribe Each Stem\")\n        print(\"=\"*80)\n        \n        stem_results = {}\n        \n        for stem_name, stem_path in stems.items():\n            print(f\"\\nðŸŽµ Transcribing {stem_name} stem...\")\n            \n            try:\n                stem_midi, stem_stats = transcribe_audio(stem_path, f\"poc_{stem_name}\")\n                stem_results[stem_name] = {\n                    'midi_path': stem_midi,\n                    'stats': stem_stats\n                }\n                \n                print(f\"   âœ… {stem_name}: {stem_stats['total_notes']} notes, {stem_stats['num_instruments']} instruments\")\n                \n            except Exception as e:\n                print(f\"   âŒ {stem_name} transcription failed: {e}\")\n                stem_results[stem_name] = {'error': str(e)}\n        \n        # Step 3.5: Create Recomposed MIDI from stems\n        print(\"\\n\" + \"=\"*80)\n        print(\"STEP 3.5: Recompose Full Mix from Stem MIDIs\")\n        print(\"=\"*80)\n        \n        recomposed_midi = None\n        recomposed_stats = None\n        \n        try:\n            # Get all stem MIDI paths\n            stem_midi_paths = {\n                stem_name: result['midi_path']\n                for stem_name, result in stem_results.items()\n                if 'midi_path' in result\n            }\n            \n            if len(stem_midi_paths) > 0:\n                # Merge all stem MIDIs\n                recomposed_midi = merge_midi_files(stem_midi_paths, \"output_midi/poc_recomposed.mid\")\n                \n                # Analyze recomposed MIDI\n                recomposed_stats, _ = analyze_midi(recomposed_midi)\n                \n                print(f\"\\nâœ… Recomposed MIDI Created:\")\n                print(f\"   Total notes: {recomposed_stats['total_notes']}\")\n                print(f\"   Instruments: {recomposed_stats['num_instruments']}\")\n            else:\n                print(\"\\nâš ï¸  No stem MIDIs to merge\")\n                \n        except Exception as e:\n            print(f\"\\nâš ï¸  Recomposition failed: {e}\")\n            import traceback\n            traceback.print_exc()\n        \n        # Step 4: Compare results\n        print(\"\\n\" + \"=\"*80)\n        print(\"STEP 4: Comparison Analysis\")\n        print(\"=\"*80)\n        \n        # Store results\n        poc_results = {\n            'audio_path': audio_path,\n            'fullmix': {\n                'midi_path': fullmix_midi,\n                'stats': fullmix_stats\n            },\n            'stems': stems,\n            'stem_results': stem_results,\n            'recomposed': {\n                'midi_path': recomposed_midi,\n                'stats': recomposed_stats\n            } if recomposed_midi else None\n        }\n        \n        # Overall comparison\n        total_stem_notes = sum(r['stats']['total_notes'] for r in stem_results.values() if 'stats' in r)\n        \n        print(f\"\\nðŸ“Š Overall Note Count:\")\n        print(f\"   Full Mix (Direct): {fullmix_stats['total_notes']} notes\")\n        print(f\"   Stems Combined (Sum): {total_stem_notes} notes\")\n        if recomposed_stats:\n            print(f\"   Recomposed Mix (Merged): {recomposed_stats['total_notes']} notes\")\n        \n        # Calculate improvements\n        improvement_sum = ((total_stem_notes - fullmix_stats['total_notes']) / fullmix_stats['total_notes'] * 100) if fullmix_stats['total_notes'] > 0 else 0\n        print(f\"\\n   Improvement (Sum): {total_stem_notes - fullmix_stats['total_notes']:+d} notes ({improvement_sum:+.1f}%)\")\n        \n        if recomposed_stats:\n            improvement_recomposed = ((recomposed_stats['total_notes'] - fullmix_stats['total_notes']) / fullmix_stats['total_notes'] * 100) if fullmix_stats['total_notes'] > 0 else 0\n            print(f\"   Improvement (Recomposed): {recomposed_stats['total_notes'] - fullmix_stats['total_notes']:+d} notes ({improvement_recomposed:+.1f}%)\")\n        \n        # Per-stem comparison\n        print(f\"\\nðŸŽ¸ Per-Stem Analysis:\")\n        for stem_name, result in stem_results.items():\n            if 'stats' in result:\n                print(f\"\\n   {stem_name.upper()}:\")\n                print(f\"      Notes detected: {result['stats']['total_notes']}\")\n                print(f\"      Instruments: {result['stats']['num_instruments']}\")\n                if result['stats']['instruments']:\n                    top_inst = sorted(result['stats']['instruments'], key=lambda x: x['notes'], reverse=True)[0]\n                    print(f\"      Top instrument: {top_inst['name']} ({top_inst['notes']} notes)\")\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"âœ… PoC TEST COMPLETE!\")\n        print(\"=\"*80)\n        print(\"\\nRun the cells below to:\")\n        print(\"   - View detailed comparison tables\")\n        print(\"   - Listen to full mix vs stems vs recomposed\")\n        print(\"   - Get recommendation based on results\")\n\n# Create button and output\npoc_button = widgets.Button(\n    description='ðŸš€ Run PoC Test',\n    button_style='success',\n    layout=widgets.Layout(width='200px', height='50px')\n)\npoc_button.on_click(run_poc_test)\n\noutput = widgets.Output()\n\nprint(\"âš ï¸  Note: This test will take 3-5 minutes depending on audio length\")\nprint(\"   - Full mix transcription: ~30s\")\nprint(\"   - Stem separation: ~30-60s\")\nprint(\"   - 4 stem transcriptions: ~2min\")\nprint(\"   - MIDI recomposition: ~1s\")\nprint(\"\\nâœ… Ready! Click the button above to start the test\")\n\ndisplay(poc_button)\ndisplay(output)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Detailed Results\n",
    "\n",
    "*Run this after the PoC test completes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if poc_results:\n    fullmix_notes = poc_results['fullmix']['stats']['total_notes']\n    total_stem_notes = sum(r['stats']['total_notes'] for r in poc_results['stem_results'].values() if 'stats' in r)\n    \n    # Get recomposed stats if available\n    recomposed_notes = None\n    if poc_results.get('recomposed') and poc_results['recomposed']:\n        recomposed_notes = poc_results['recomposed']['stats']['total_notes']\n    \n    improvement_sum = ((total_stem_notes - fullmix_notes) / fullmix_notes * 100) if fullmix_notes > 0 else 0\n    improvement_recomposed = ((recomposed_notes - fullmix_notes) / fullmix_notes * 100) if recomposed_notes and fullmix_notes > 0 else None\n    \n    print(\"=\"*80)\n    print(\"ðŸ“Š PoC RESULTS SUMMARY\")\n    print(\"=\"*80)\n    \n    print(f\"\\nðŸ“ˆ Note Count Comparison:\")\n    print(f\"   Full Mix (Direct): {fullmix_notes} notes\")\n    print(f\"   Stems Combined (Sum): {total_stem_notes} notes\")\n    if recomposed_notes:\n        print(f\"   Recomposed Mix (Merged): {recomposed_notes} notes\")\n    \n    print(f\"\\nðŸŽ¯ Improvement Analysis:\")\n    print(f\"   Sum of Stems: {total_stem_notes - fullmix_notes:+d} notes ({improvement_sum:+.1f}%)\")\n    if improvement_recomposed is not None:\n        print(f\"   Recomposed Mix: {recomposed_notes - fullmix_notes:+d} notes ({improvement_recomposed:+.1f}%)\")\n    \n    # Use recomposed improvement if available, otherwise use sum\n    primary_improvement = improvement_recomposed if improvement_recomposed is not None else improvement_sum\n    \n    print(f\"\\nðŸ’¡ Key Insight:\")\n    if recomposed_notes and recomposed_notes == total_stem_notes:\n        print(\"   âœ… Recomposed MIDI matches sum of stems (no overlap/duplicate notes)\")\n    elif recomposed_notes and recomposed_notes != total_stem_notes:\n        print(f\"   âš ï¸  Recomposed MIDI differs from sum ({total_stem_notes - recomposed_notes:+d} notes)\")\n        print(\"       This could indicate overlapping notes or timing differences\")\n    \n    print(\"\\n\" + \"=\"*80)\n    \n    if primary_improvement > 10:\n        print(\"âœ… RECOMMENDATION: PROCEED WITH IDEA 1 (Fine-tuning)\")\n        print(\"\\n   Stems approach validates the hypothesis!\")\n        print(\"   Fine-tuning will likely add another 10-15% improvement.\")\n        print(\"   Expected total improvement: 20-30%\\n\")\n        print(\"   Next steps:\")\n        print(\"   1. Read YOURMT3_FINETUNING_GUIDE.md\")\n        print(\"   2. Download Slakh2100 dataset (~1TB)\")\n        print(\"   3. Start fine-tuning bass model (3-5 days)\")\n        print(\"   4. Continue with other stems if bass succeeds\")\n    elif primary_improvement > 5:\n        print(\"âš ï¸  RECOMMENDATION: INVESTIGATE FURTHER\")\n        print(\"\\n   Moderate improvement detected.\")\n        print(\"   Check stem quality and per-stem results.\\n\")\n        print(\"   Action items:\")\n        print(\"   1. Listen to separated stems (next cell)\")\n        print(\"   2. Check which stems work best\")\n        print(\"   3. Try different Demucs model: mdx_extra\")\n        print(\"   4. Re-run PoC with better separation\")\n    else:\n        print(\"âŒ RECOMMENDATION: RECONSIDER APPROACH\")\n        print(\"\\n   Stems not providing expected benefit.\")\n        print(\"   Fine-tuning may not help.\\n\")\n        print(\"   Alternatives:\")\n        print(\"   1. Investigate Demucs quality (listen to stems)\")\n        print(\"   2. Try Idea 2 (instrument matching)\")\n        print(\"   3. Use different stem separation (Spleeter, Open-Unmix)\")\n        print(\"   4. Hybrid approach: stems for specific instruments only\")\n    \n    print(\"\\n\" + \"=\"*80)\nelse:\n    print(\"âš ï¸  Run the PoC test first (Cell 6)!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Piano Roll Visualization\n",
    "\n",
    "*Visual comparison of MIDI transcriptions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if poc_results:\n    print(\"ðŸŽ¹ Piano Roll Visualizations with MIDI Playback\")\n    print(\"=\"*80)\n    \n    def plot_piano_roll(midi_path, title, ax=None):\n        \"\"\"Plot piano roll from MIDI file\"\"\"\n        midi = pretty_midi.PrettyMIDI(midi_path)\n        \n        if ax is None:\n            fig, ax = plt.subplots(figsize=(16, 6))\n        \n        # Get all notes\n        notes_to_plot = []\n        colors = plt.cm.tab20.colors\n        \n        for inst_idx, inst in enumerate(midi.instruments):\n            color = colors[inst_idx % len(colors)]\n            for note in inst.notes:\n                notes_to_plot.append({\n                    'start': note.start,\n                    'end': note.end,\n                    'pitch': note.pitch,\n                    'velocity': note.velocity,\n                    'color': color,\n                    'instrument': inst.program,\n                    'is_drum': inst.is_drum\n                })\n        \n        # Plot notes\n        for note_info in notes_to_plot:\n            ax.add_patch(\n                plt.Rectangle(\n                    (note_info['start'], note_info['pitch']),\n                    note_info['end'] - note_info['start'],\n                    1,\n                    facecolor=note_info['color'],\n                    edgecolor='black',\n                    linewidth=0.5,\n                    alpha=0.7\n                )\n            )\n        \n        ax.set_xlim(0, midi.get_end_time())\n        ax.set_ylim(20, 108)  # Piano range\n        ax.set_xlabel('Time (seconds)', fontsize=12)\n        ax.set_ylabel('MIDI Pitch', fontsize=12)\n        ax.set_title(title, fontsize=14, fontweight='bold')\n        ax.grid(True, alpha=0.3)\n        \n        # Add note count\n        total_notes = len(notes_to_plot)\n        ax.text(0.98, 0.02, f'{total_notes} notes', \n                transform=ax.transAxes,\n                ha='right', va='bottom',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n                fontsize=10)\n        \n        return ax\n    \n    print(\"\\nðŸ“Š Generating visualizations...\")\n    \n    # 1. Full Mix\n    print(\"\\n\" + \"=\"*80)\n    print(\"ðŸŽµ FULL MIX (Baseline - Direct Transcription)\")\n    print(\"=\"*80)\n    \n    fig, ax = plt.subplots(figsize=(18, 4))\n    plot_piano_roll(poc_results['fullmix']['midi_path'], \n                    'ðŸŽµ Full Mix Transcription (Direct)', ax)\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nðŸ”Š Audio Playback:\")\n    audio, sr = midi_to_audio(poc_results['fullmix']['midi_path'])\n    if audio is not None:\n        display(Audio(audio, rate=sr))\n    \n    # 2. Each Stem\n    stem_names = ['drums', 'bass', 'other', 'vocals']\n    stem_emojis = {'drums': 'ðŸ¥', 'bass': 'ðŸŽ¸', 'other': 'ðŸŽ¹', 'vocals': 'ðŸŽ¤'}\n    \n    for stem_name in stem_names:\n        if stem_name in poc_results['stem_results'] and 'midi_path' in poc_results['stem_results'][stem_name]:\n            print(\"\\n\" + \"=\"*80)\n            print(f\"{stem_emojis[stem_name]} {stem_name.upper()} STEM\")\n            print(\"=\"*80)\n            \n            fig, ax = plt.subplots(figsize=(18, 4))\n            plot_piano_roll(poc_results['stem_results'][stem_name]['midi_path'],\n                            f'{stem_emojis[stem_name]} {stem_name.title()} Stem Transcription',\n                            ax)\n            plt.tight_layout()\n            plt.show()\n            \n            print(\"\\nðŸ”Š Audio Playback:\")\n            audio, sr = midi_to_audio(poc_results['stem_results'][stem_name]['midi_path'])\n            if audio is not None:\n                display(Audio(audio, rate=sr))\n    \n    # 3. Recomposed Mix\n    if poc_results.get('recomposed') and poc_results['recomposed'] and poc_results['recomposed']['midi_path']:\n        print(\"\\n\" + \"=\"*80)\n        print(\"ðŸŽ¼ RECOMPOSED MIX (Stems Merged Back Together)\")\n        print(\"=\"*80)\n        \n        fig, ax = plt.subplots(figsize=(18, 4))\n        plot_piano_roll(poc_results['recomposed']['midi_path'],\n                        'ðŸŽ¼ Recomposed Mix (All Stems Merged)',\n                        ax)\n        plt.tight_layout()\n        plt.show()\n        \n        print(\"\\nðŸ”Š Audio Playback:\")\n        audio, sr = midi_to_audio(poc_results['recomposed']['midi_path'])\n        if audio is not None:\n            display(Audio(audio, rate=sr))\n        \n        print(\"\\nðŸ’¡ Comparison:\")\n        fullmix_notes = poc_results['fullmix']['stats']['total_notes']\n        recomposed_notes = poc_results['recomposed']['stats']['total_notes']\n        diff = recomposed_notes - fullmix_notes\n        print(f\"   Full Mix (Direct): {fullmix_notes} notes\")\n        print(f\"   Recomposed (Stems Merged): {recomposed_notes} notes\")\n        print(f\"   Difference: {diff:+d} notes ({(diff/fullmix_notes*100):+.1f}%)\")\n        \n        if diff > 0:\n            print(\"\\n   âœ… Recomposed mix captured MORE notes than direct transcription\")\n            print(\"      This suggests stem separation helps the model detect more musical content\")\n        elif diff < 0:\n            print(\"\\n   âš ï¸  Recomposed mix has FEWER notes than direct transcription\")\n            print(\"      This might indicate stem separation quality issues\")\n        else:\n            print(\"\\n   âž¡ï¸  Same number of notes, but possibly different timing or instruments\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"âœ… All visualizations generated!\")\n    print(\"=\"*80)\n    print(\"\\nðŸ’¡ Visual Insights:\")\n    print(\"   - Vertical position = pitch (higher = higher note)\")\n    print(\"   - Horizontal position = time\")\n    print(\"   - Rectangle width = note duration\")\n    print(\"   - Colors = different instruments\")\n    print(\"   - Compare density to see which stem has more detected notes\")\n    print(\"   - Listen to MIDI playback to verify transcription quality\")\n    print(\"\\nðŸ“Š Comparison Strategy:\")\n    print(\"   1. Full Mix (Direct) = baseline transcription of original audio\")\n    print(\"   2. Individual Stems = isolated instrument transcriptions\")\n    print(\"   3. Recomposed Mix = all stems merged back together\")\n    print(\"   â†’ Recomposed should capture more musical information than Full Mix\")\n    \nelse:\n    print(\"âš ï¸  Run the PoC test first (Cell 6)!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Listen to Stems Quality\n",
    "\n",
    "*Verify stem separation quality*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if poc_results:\n",
    "    print(\"ðŸŽ§ Audio Playback: Original vs Stems\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Original audio\n",
    "    print(\"\\nðŸŽµ Original Full Mix:\")\n",
    "    waveform, sr = torchaudio.load(poc_results['audio_path'])\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    display(Audio(waveform.numpy(), rate=sr))\n",
    "    \n",
    "    # Each stem\n",
    "    for stem_name, stem_path in poc_results['stems'].items():\n",
    "        print(f\"\\nðŸŽ¸ {stem_name.title()} Stem:\")\n",
    "        waveform, sr = torchaudio.load(stem_path)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        display(Audio(waveform.numpy(), rate=sr))\n",
    "else:\n",
    "    print(\"âš ï¸  Run the PoC test first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results\n",
    "\n",
    "*Export PoC results to JSON*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if poc_results:\n    # Save results as JSON\n    results_summary = {\n        'test_file': os.path.basename(poc_results['audio_path']),\n        'fullmix': {\n            'notes': poc_results['fullmix']['stats']['total_notes'],\n            'instruments': poc_results['fullmix']['stats']['num_instruments'],\n            'duration': poc_results['fullmix']['stats']['duration']\n        },\n        'stems': {}\n    }\n    \n    for stem_name, result in poc_results['stem_results'].items():\n        if 'stats' in result:\n            results_summary['stems'][stem_name] = {\n                'notes': result['stats']['total_notes'],\n                'instruments': result['stats']['num_instruments']\n            }\n    \n    # Add recomposed results if available\n    if poc_results.get('recomposed') and poc_results['recomposed']:\n        results_summary['recomposed'] = {\n            'notes': poc_results['recomposed']['stats']['total_notes'],\n            'instruments': poc_results['recomposed']['stats']['num_instruments']\n        }\n    \n    # Calculate overall improvement\n    fullmix_notes = results_summary['fullmix']['notes']\n    total_stem_notes = sum(s['notes'] for s in results_summary['stems'].values())\n    improvement_sum = ((total_stem_notes - fullmix_notes) / fullmix_notes * 100) if fullmix_notes > 0 else 0\n    \n    results_summary['improvement'] = {\n        'absolute_sum': total_stem_notes - fullmix_notes,\n        'percentage_sum': improvement_sum\n    }\n    \n    # Add recomposed improvement if available\n    if 'recomposed' in results_summary:\n        recomposed_notes = results_summary['recomposed']['notes']\n        improvement_recomposed = ((recomposed_notes - fullmix_notes) / fullmix_notes * 100) if fullmix_notes > 0 else 0\n        results_summary['improvement']['absolute_recomposed'] = recomposed_notes - fullmix_notes\n        results_summary['improvement']['percentage_recomposed'] = improvement_recomposed\n    \n    # Save to file\n    output_path = os.path.join(original_dir, 'poc_results.json')\n    with open(output_path, 'w') as f:\n        json.dump(results_summary, f, indent=2)\n    \n    print(f\"âœ… PoC results saved to: {output_path}\")\n    print(f\"\\nðŸ“„ Summary:\")\n    print(json.dumps(results_summary, indent=2))\nelse:\n    print(\"âš ï¸  Run the PoC test first!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ PoC Complete!\n",
    "\n",
    "**If improvement >10%**: Proceed with fine-tuning (see `YOURMT3_FINETUNING_GUIDE.md`)\n",
    "\n",
    "**If improvement 5-10%**: Investigate stem quality, try different Demucs model\n",
    "\n",
    "**If improvement <5%**: Consider Idea 2 (instrument matching) or improve stem separation\n",
    "\n",
    "---\n",
    "\n",
    "*Assisted by Claude Code*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}