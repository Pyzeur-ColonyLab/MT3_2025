# YourMT3 Analysis et DÃ©cision d'IntÃ©gration

Date: 2025-10-01
Status: Analysis Complete - DÃ©cision Requise

## ğŸ“Š RÃ©sumÃ© ExÃ©cutif

**ProblÃ¨me actuel**: MT3 avec checkpoint T5X converti gÃ©nÃ¨re uniquement le token 1133 â†’ 0 notes
**Root cause**: Vocabulaire mismatch (Model: 1536 tokens vs Decoder: 1491 tokens)
**Alternative**: YourMT3+ (2024) dÃ©mo HF fonctionne "ok (5/10)"

## ğŸ” Analyse YourMT3+

### Architecture

**YourMT3 != MT3**: ComplexitÃ© significativement plus Ã©levÃ©e

```
YourMT3 (2024)                    vs        MT3 (2021)
â”œâ”€ Encoder                                  â”œâ”€ Encoder
â”‚  â”œâ”€ T5-small (option)                    â”‚  â””â”€ T5-small
â”‚  â”œâ”€ Perceiver-TF (dÃ©mo)                  â”‚
â”‚  â””â”€ Conformer (option)                   â”‚
â”œâ”€ Decoder                                  â”œâ”€ Decoder
â”‚  â”œâ”€ Multi-channel T5 (dÃ©mo)              â”‚  â””â”€ T5-small standard
â”‚  â””â”€ Single-channel T5                    â”‚
â”œâ”€ Mixture of Experts (MoE)                 â””â”€ (pas de MoE)
â”œâ”€ Multi-instrument channels
â””â”€ Lightning + custom task manager
```

### DÃ©pendances

**YourMT3 Requirements**:
```
pytorch-lightning>=2.2.1
transformers==4.45.1
torch, torchaudio
librosa, einops
mido
```

**Notre code actuel**:
```
torch, torchaudio
pretty_midi (pour MIDI)
librosa (pour audio)
```

### Fichiers ClÃ©s

```
yourmt3_space/
â”œâ”€â”€ amt/src/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ ymt3.py                 # 800+ lignes, PyTorch Lightning
â”‚   â”‚   â”œâ”€â”€ t5mod.py                # T5 modifiÃ©
â”‚   â”‚   â”œâ”€â”€ perceiver_mod.py        # Perceiver-TF encoder
â”‚   â”‚   â”œâ”€â”€ conformer_mod.py        # Conformer encoder
â”‚   â”‚   â””â”€â”€ lm_head.py              # Classification head
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ config.py               # Configs audio/model/shared
â”‚   â”‚   â”œâ”€â”€ task.py                 # Task manager
â”‚   â”‚   â””â”€â”€ vocabulary.py           # Vocabulaires
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ task_manager.py         # Tokenization/detokenization
â”‚       â”œâ”€â”€ note2event.py           # Note â†’ Event conversion
â”‚       â””â”€â”€ event2note.py           # Event â†’ Note conversion
â”œâ”€â”€ model_helper.py                  # Load model + inference
â””â”€â”€ requirements.txt
```

### ModÃ¨le UtilisÃ© dans la DÃ©mo

**Checkpoint**: `mc13_256_g4_all_v7_mt3f_sqr_rms_moe_wf4_n8k2_silu_rope_rp_b36_nops@last.ckpt`

**Configuration**:
- Encoder: Perceiver-TF + Mixture of Experts (2/8)
- Decoder: Multi-channel T5-small
- Tokenizer: MT3 tokens + extension Singing
- Audio: Spec (pas melspec), hop=300, 512 mel bins
- Training: Cross-dataset stem augmentation, no pitch-shifting
- Precision: BF16-mixed training, FP16 inference

### Pipeline d'InfÃ©rence

```python
# 1. Model Loading (model_helper.py:25-123)
model = YourMT3(
    audio_cfg=audio_cfg,
    model_cfg=model_cfg,
    shared_cfg=shared_cfg,
    task_manager=tm,
    ...
).to(device)
checkpoint = torch.load(checkpoint_path)
model.load_state_dict(checkpoint['state_dict'])

# 2. Audio Preprocessing (model_helper.py:131-136)
audio, sr = torchaudio.load(audio_file)
audio = torch.mean(audio, dim=0).unsqueeze(0)  # Mono
audio = torchaudio.functional.resample(audio, sr, 16000)
audio_segments = slice_padded_array(audio, input_frames, input_frames)
audio_segments = torch.from_numpy(audio_segments).to(device).unsqueeze(1)

# 3. Model Inference (model_helper.py:141)
pred_token_arr, _ = model.inference_file(bsz=8, audio_segments=audio_segments)

# 4. Post-processing (model_helper.py:151-158)
for ch in range(num_channels):
    zipped_note_events_and_tie = task_manager.detokenize_list_batches(pred_token_arr_ch)
    pred_notes_ch = merge_zipped_note_events_and_ties_to_notes(zipped_note_events_and_tie)
    pred_notes_in_file.append(pred_notes_ch)
pred_notes = mix_notes(pred_notes_in_file)

# 5. Write MIDI (model_helper.py:161-162)
write_model_output_as_midi(pred_notes, output_dir, track_name, midi_inverse_vocab)
```

## ğŸ¯ Options d'IntÃ©gration

### Option A: IntÃ©gration ComplÃ¨te YourMT3

**Approche**: Copier `amt/src/` entiÃ¨rement dans notre projet

**Avantages**:
- âœ… AccÃ¨s Ã  l'architecture complÃ¨te
- âœ… Support multi-instruments
- âœ… QualitÃ© prouvÃ©e (dÃ©mo fonctionne)

**InconvÃ©nients**:
- âŒ **~50 fichiers Python Ã  intÃ©grer**
- âŒ DÃ©pendances lourdes (PyTorch Lightning, etc.)
- âŒ ComplexitÃ© Ã©levÃ©e (800+ lignes pour le modÃ¨le seul)
- âŒ Maintenance difficile
- âŒ NÃ©cessite tÃ©lÃ©chargement checkpoint (~500MB)

**Effort**: ğŸ”´ 3-5 jours

### Option B: Wrapper SimplifiÃ© YourMT3

**Approche**: CrÃ©er `yourmt3_inference.py` qui appelle le code existant

**Avantages**:
- âœ… RÃ©utilise le code YourMT3 tel quel
- âœ… Moins de duplication
- âœ… Mise Ã  jour facile

**InconvÃ©nients**:
- âŒ DÃ©pend toujours de `amt/src/`
- âŒ NÃ©cessite configuration complexe
- âŒ Pas de contrÃ´le sur l'architecture

**Effort**: ğŸŸ¡ 1-2 jours

### Option C: Checkpoint MT3 Officiel depuis HuggingFace

**Approche**: Chercher checkpoint MT3 prÃ©-entraÃ®nÃ© sur HF Hub

**Status**:
- Google bucket vide: âŒ `gs://magentadata/models/mt3/checkpoints`
- HuggingFace: ğŸ” Ã€ vÃ©rifier

**Avantages**:
- âœ… Notre code MT3 dÃ©jÃ  fonctionnel (sauf checkpoint)
- âœ… Architecture simple et comprise
- âœ… Pas de dÃ©pendances lourdes

**InconvÃ©nients**:
- âš ï¸ Checkpoint officiel peut ne pas exister sur HF
- âš ï¸ QualitÃ© possiblement infÃ©rieure Ã  YourMT3

**Effort**: ğŸŸ¢ <1 jour SI checkpoint existe

### Option D: Basic-Pitch (Alternative Lightweight)

**Approche**: Utiliser Spotify Basic-Pitch au lieu de MT3/YourMT3

**Model**: `spotify/basic-pitch` sur HuggingFace

**Avantages**:
- âœ… Architecture simple (CNN + RNN)
- âœ… API HF transformers standard
- âœ… LÃ©ger et rapide
- âœ… Maintenance active

**InconvÃ©nients**:
- âš ï¸ Moins prÃ©cis que MT3/YourMT3
- âš ï¸ Pas multi-instruments natif
- âš ï¸ Architecture diffÃ©rente (pas T5)

**Effort**: ğŸŸ¢ <1 jour

## ğŸ§ª Tests EffectuÃ©s

### MT3 Converti (Notre Code)
```
Checkpoint: mt3_converted_fixed.pth
Audio: 02.HowardShore-TheShire.flac (149s)
RÃ©sultat:
  âŒ Generated 2048 tokens
  âŒ Unique tokens: 3 (only [0, 1133, 1133...])
  âŒ Notes detected: 0
  âŒ MIDI file: vide
Root cause: Vocabulary mismatch (1536 vs 1491 tokens)
```

### YourMT3 Demo HF
```
Model: YPTF.MoE+Multi (noPS)
Interface: https://huggingface.co/spaces/mimbres/YourMT3
RÃ©sultat:
  âœ… Fonctionne "ok (5/10)"
  âœ… GÃ©nÃ¨re des notes
  âœ… MIDI jouable
Limitation: YouTube bloquÃ©, exemples prÃ©-transcrits seulement
```

## ğŸ“Š Comparaison Quantitative

| CritÃ¨re | MT3 (notre code) | YourMT3+ Full | YourMT3 Wrapper | Basic-Pitch |
|---------|------------------|---------------|-----------------|-------------|
| **Effort** | ğŸŸ¢ Code existe | ğŸ”´ 3-5 jours | ğŸŸ¡ 1-2 jours | ğŸŸ¢ <1 jour |
| **ComplexitÃ©** | ğŸŸ¢ Simple | ğŸ”´ TrÃ¨s Ã©levÃ©e | ğŸŸ¡ Moyenne | ğŸŸ¢ Simple |
| **QualitÃ©** | âŒ 0 notes | âœ… 5/10 | âœ… 5/10 | âš ï¸ 3-4/10 |
| **Maintenance** | ğŸŸ¢ Facile | ğŸ”´ Difficile | ğŸŸ¡ Moyenne | ğŸŸ¢ Facile |
| **DÃ©pendances** | ğŸŸ¢ LÃ©gÃ¨res | ğŸ”´ Lourdes | ğŸŸ¡ Moyennes | ğŸŸ¢ LÃ©gÃ¨res |
| **Multi-instruments** | âš ï¸ Possible | âœ… Natif | âœ… Natif | âŒ LimitÃ© |
| **Checkpoint** | âŒ Invalide | âœ… Existe | âœ… Existe | âœ… HF Hub |

## ğŸ’¡ Recommandation

### StratÃ©gie Hybride (Option C â†’ B â†’ D)

**Phase 1: VÃ©rifier HF Hub pour MT3 (30 min)**
```bash
# Chercher checkpoint MT3 officiel
huggingface-cli search "MT3" --filter "model"
huggingface-cli search "music transcription" --filter "model"
```
- SI trouvÃ© â†’ Utiliser avec notre code existant âœ…
- SI pas trouvÃ© â†’ Phase 2

**Phase 2: YourMT3 Wrapper Minimal (1-2 jours)**
- Copier `amt/src/` dans `external/yourmt3/`
- CrÃ©er `yourmt3_inference.py` wrapper simple
- TÃ©lÃ©charger checkpoint depuis HF Space
- Tester avec "The Shire"
- SI qualitÃ© acceptable (â‰¥4/10) â†’ DONE âœ…
- SI qualitÃ© insuffisante â†’ Phase 3

**Phase 3: Fallback Basic-Pitch (1 jour)**
- ImplÃ©menter `basic_pitch_inference.py`
- Comparer avec YourMT3
- Choisir le meilleur compromis qualitÃ©/complexitÃ©

## ğŸ“ Actions ImmÃ©diates

1. âœ… Clone YourMT3 Space: **DONE**
2. âœ… Analyse architecture: **DONE**
3. â³ **NEXT**: Rechercher checkpoint MT3 sur HF Hub
4. â³ Si pas trouvÃ©: CrÃ©er wrapper YourMT3
5. â³ Tester avec "The Shire"
6. â³ Documenter rÃ©sultats

## ğŸ”— RÃ©fÃ©rences

- **YourMT3 Paper**: [arxiv.org/abs/2407.04822](https://arxiv.org/abs/2407.04822)
- **YourMT3 HF Space**: [huggingface.co/spaces/mimbres/YourMT3](https://huggingface.co/spaces/mimbres/YourMT3)
- **YourMT3 Model**: [huggingface.co/mimbres/YourMT3](https://huggingface.co/mimbres/YourMT3)
- **YourMT3 GitHub**: [github.com/mimbres/YourMT3](https://github.com/mimbres/YourMT3) (empty repo)
- **MT3 Original**: Google Magenta (bucket vide)
- **Basic-Pitch**: [huggingface.co/spotify/basic-pitch](https://huggingface.co/spotify/basic-pitch)

---

## âš ï¸ DÃ©cision Requise

**Question pour l'utilisateur**:

Quelle approche prÃ©fÃ©rez-vous ?

1. **Option Rapide** ğŸŸ¢: Rechercher MT3 sur HF Hub (30 min) puis Basic-Pitch si Ã©chec
2. **Option QualitÃ©** ğŸŸ¡: YourMT3 Wrapper (~2 jours) pour 5/10 qualitÃ©
3. **Option ComplÃ¨te** ğŸ”´: IntÃ©gration YourMT3 full (~5 jours) pour contrÃ´le total

**Ma recommandation**: Option Rapide â†’ QualitÃ© (stratÃ©gie hybride)
